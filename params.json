{
  "name": "Sentiment-analysis-on-movie-reviews",
  "tagline": "",
  "body": "# Background description\r\nNatural language processing (NLP) is an important field of computer science, and it is a significant research branch of data science. Speaking of the world we are living in recent years, we are surrounded with varieties of reviews which are helpful when we are making decisions. In this project, our goal is to benchmark our sentiment-analysis ideas on the Rotten Tomatoes movie review dataset. We are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive, which are corresponding to 0, 1, 2, 3, and 4 in the train.tsv file. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.\r\n\r\n# Data analysis\r\nHere the dataset is the Rotten Tomatoes movie review, which is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee. Specifically, for train.tsv file, each record consists of four items, i.e., PhraseId, SentenceID, Phrase, and Sentiment. Corresponding to every sentence, there are all kinds of continuous phrase sequences(same SentenceID, incrementing PhraseIds). And 0 - 4 stands for the sentiment label in the sentiment column. And there are 156060 records in total. \r\n\r\nFor test.tsv file, each record consists of three items, i.e., PhraseId, SentenceID, and Phrase.  And there are 66293 records to be labeled in total. \r\n\r\n# Method analysis\r\nWe decided to deeply analyze two methods, i.e., the Deep Learning for sentiment analysis from Stanford NLP Lab and the ordinary machine learning method(based on useful libraries and classifiers).\r\n\r\n### _Method1 -- based on machine learning libraries_\r\nOur learning and modifications are based on: [here](https://github.com/rafacarrascosa/samr/tree/develop/samr)\r\n\r\nThe machine learning method package is based on _scikit-learn_ and _nltk_. Basically, it contains a configuration file that is in JSON format, which determines how the _scikit-learn_ pipeline is going to be built and other hyperparameters. We can adjust the classifier using and relevant arguments setting, which may bring better accuracy on the test data file. \r\nThe predictor.py is the main module, and within it, the PhraseSentimentPredictor is the class that does the prediction and therefore one of the main entry points to the library.  A configurable main classifier is trained with the following two features: a. The decision functions use a one-versus-others scheme picking bag-of-words as features. It's a classifier inside a classifier. b. The amounts of \"positive\" and \"negative\" words in a phrase are dictated by the Harvard Inquirer sentiment lexicon. \r\n\r\nBased on the current code, the valid values of the main classifier used are “sgd”(SGDClassifier), “knn”(KNeighborsClassifier), “svc”(SVC), and “randomforest”(RandomForestClassifier). Corresponding to each classifier, there is a dictionary to be passed as arguments. And we can also use other classifiers by importing them, as long as they have been implemented in scikit-learn library. \r\n\r\nFor example, let’s use “randomforest”, and the arguments are: {\"n_estimators\": 100, \"min_samples_leaf\":20, \"n_jobs\":-1}. \r\n“n_estimators” is the number of trees in the forest. The larger the better, but also the longer it will take to compute. But the results will stop getting significantly better beyond a critical number of trees. \"min_samples_leaf\" means the amount of samples for a leaf node, i.e. the trees are fully developed when its value is 1.  And we use “n_jobs” to denote the parallel construction of the trees. When “n_jobs” = k, computations are partitioned into k jobs, and run on k cores of the machine. If n_jobs=-1 then all cores available on the machine are used. \r\n\r\n### Experiment result\r\nYou can see the details here: [https://docs.google.com/document/d/1RC7xrjuDkv20JhW36B17u6o3WgAalG7mtf9SO982FJI/edit?usp=sharing](click me)\r\n\r\n### _Method2 -- based on deep learning_\r\n### Background description\r\nThe increasing popularity of Internet and social media exposed us to the massive information in our daily life. To make it more efficient to review the message and extract the useful data from it, message analysis can be a big topic to work on. Semantic word spaces or semantic vector spaces are used to represent the unit information when analysing the data. The sentences are divided into different semantic vector spaces and labeled with classes like negative and positive. However, this kind of method sometimes can’t yield ideal result since they ignore the order of each words. In that case, The Stanford Sentiment Treebank is developed to improve the overall accuracy in message analysis system. Based on the method we mentioned above, they still divided the sentence but they take the order of words into account and form a tree structure, which is more organized and turned out to have the best performance ever.\r\n\r\n### Method analysis\r\n* The treebank\r\nThe new Sentiment Treebank includes labels for every syntactically plausible phrase in thousands of sentences, which make it easier to train and evaluate compositional models. The dataset which the method uses includes 10662 sentences and half of them are positive and the other half are negative. They did much data pre-processing work to make sure that the training results can be more accurate. They deleted the label which are not English. They created a label interface to help them categorize each unit information. \r\n\r\n* Recursive Neural Models\r\n1. RNN: Recursive Neural Network\r\n* The simplest member of this family of neural network models is the standard recursive neural network . For tree structure of RNN, it is determined that which parent has all their children calculated. Although RNN ignores the reconstruction loss, in practical experiment, the actual loss is small enough to be ignored.\r\n2. MV-RNN: Matrix-Vector RNN\r\n* The main idea of the MV-RNN is to represent every word and longer phrase in a parse tree as both a vector and a matrix. When two constituents are combined, the matrix of one is multiplied with the vector of the other one, and vice versa. In this model, each word’s matrix is initialized as a d×d identity matrix, plus a small amount of Gaussian noise, where the parameter can contribute to less error.\r\n3. RNTN:Recursive Neural Tensor Network\r\n* To solve the problem of increasing number of parameters concerning the MV-RNN models and the single interaction with input vectors concerning the RNN, RNTN is invented. The main idea is to use the same, tensor-based composition function for all nodes.\r\n\r\n* Experiment\r\nTheir test analysis can be divided into two steps. One is the analysis focusing on the data set, and the other one is the analysis based on two linguistic phenomena that are important in sentiment.\r\n1. They used fine-grained sentiment for all phrases and binary sentiment. The results they got showed that the new treebank structure remarkably improved the overall accuracy. \r\n2. They then analyzed the different models focusing on Contrastive Conjunction, High Level Negation and Most Positive and Negative Phrases. Those are all common linguistic phenomena we can see in our daily life. The sub-dataset building and testing using different models are talked above.\r\n\r\n### Demo results\r\n[click here for details](https://docs.google.com/document/d/1RC7xrjuDkv20JhW36B17u6o3WgAalG7mtf9SO982FJI/edit?usp=sharing)\r\n\r\n### Conclusion\r\nOverall, this method introduced a brand new structure (Stanford treebank) to the sentiment analysis field. It pushed the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines.\r\n\r\n# Comparison of two methods and reflection\r\nThe first method is based on relatively mature machine learning package, using different classifiers to label the sentiments. As far as we have experimented, the accuracy is less than 70%. However, since it is based on popular and mature ML packages, it is easier to understand and use in practical. \r\n\r\nThe second deep learning method brings the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases. The achievement of this method inspired us to think things differently especially when we consider the data we want to input. A new way to process the data can bring new idea of algorithm and probable improvement overall.\r\n\r\n# Contact\r\nThis project is maintained by Yuntuo Wang, and you can search username \"yuntuowang\" on Github. If you get any concerns or questions, feel free to contact me. \r\n\r\n_Contact info:_ \r\n\r\n* Yuntuo Wang       yuntuowang2015@u.northwestern.edu\r\n* Peiyu Wang        peiyuwang2016@u.northwestern.edu",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}